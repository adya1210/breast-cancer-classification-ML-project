{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6504514-52cd-4748-a355-58cc424c949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and GPU Check\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da86362-abf7-4b82-a36e-68ebaed366fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b838708-2c39-42e6-a0b9-259050e0bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4bba48-89f5-4320-8174-956fe8df4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed project in Google Colab with dataset loaded in Google Drive\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = '/content/drive/MyDrive/archive (2)/BreaKHis_v1/BreaKHis_v1/histology_slides/breast'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc49fdf-ac2e-44db-abce-75e960cb5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class BreakHisDataset(Dataset):\n",
    "    \"\"\"Custom dataset for BreakHis histopathology images\"\"\"\n",
    "    def __init__(self, base_path, magnification='400X', transform=None):\n",
    "        self.base_path = base_path\n",
    "        self.magnification = magnification\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_name, label in [('benign', 0), ('malignant', 1)]:\n",
    "            class_path = os.path.join(base_path, class_name)\n",
    "            for root, dirs, files in os.walk(class_path):\n",
    "                if magnification in root:\n",
    "                    for file in files:\n",
    "                        if file.endswith('.png'):\n",
    "                            self.images.append(os.path.join(root, file))\n",
    "                            self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0ea6b-1521-477a-926f-1c3089a06236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f3d78-8e2d-4d44-8930-e2eac2baf890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Magnifications\n",
    "datasets_all_mags = {}\n",
    "\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    dataset = BreakHisDataset(base_path, magnification=mag, transform=transform)\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    labels = dataset.labels\n",
    "\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, stratify=labels)\n",
    "    temp_labels = [labels[i] for i in temp_idx]\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=42, stratify=temp_labels)\n",
    "\n",
    "    train_dataset = Subset(dataset, train_idx)\n",
    "    val_dataset = Subset(dataset, val_idx)\n",
    "    test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    datasets_all_mags[mag] = {\n",
    "        'dataset': dataset,\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'test_loader': test_loader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2128abb-75d6-4e42-9c38-e93e40e4b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 Feature Extractor\n",
    "resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "def extract_features(dataloader, extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            feats = extractor(images)\n",
    "            feats = feats.view(feats.size(0), -1)\n",
    "            features.append(feats.cpu().numpy())\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    return np.vstack(features), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1999141-2240-4c4f-a8f8-41125073d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features for All Magnifications\n",
    "all_features = {}\n",
    "\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    train_loader = datasets_all_mags[mag]['train_loader']\n",
    "    val_loader = datasets_all_mags[mag]['val_loader']\n",
    "    test_loader = datasets_all_mags[mag]['test_loader']\n",
    "\n",
    "    X_train, y_train = extract_features(train_loader, feature_extractor)\n",
    "    X_val, y_val = extract_features(val_loader, feature_extractor)\n",
    "    X_test, y_test = extract_features(test_loader, feature_extractor)\n",
    "\n",
    "    all_features[mag] = {\n",
    "        'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val': X_val, 'y_val': y_val,\n",
    "        'X_test': X_test, 'y_test': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce24ba-b17b-4fb3-9e19-75e7f4f2af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Traditional ML Models\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "models_params = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "        'params': {'C': [0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
    "    },\n",
    "    'Linear SVM': {\n",
    "        'model': SVC(kernel='linear', class_weight='balanced', random_state=42),\n",
    "        'params': {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    },\n",
    "    'RBF SVM': {\n",
    "        'model': SVC(kernel='rbf', class_weight='balanced', random_state=42),\n",
    "        'params': {'C': [0.1, 1, 10], 'gamma': ['scale', 0.001, 0.01, 0.1]}\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81d808-b86b-44de-9da3-c091f710e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Traditional ML Models on All Magnifications\n",
    "all_results = {}\n",
    "\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    X_train = all_features[mag]['X_train']\n",
    "    y_train = all_features[mag]['y_train']\n",
    "    X_test = all_features[mag]['X_test']\n",
    "    y_test = all_features[mag]['y_test']\n",
    "\n",
    "    mag_results = {}\n",
    "\n",
    "    for name, config in models_params.items():\n",
    "        grid_search = GridSearchCV(config['model'], config['params'], \n",
    "                                   cv=5, scoring=f1_scorer, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        mag_results[name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred)\n",
    "        }\n",
    "\n",
    "    all_results[mag] = mag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87388b-cbc7-4bbf-870e-5a6d3321b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Architecture\n",
    "class BreastCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BreastCancerCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc02c5-1101-4141-b50b-418af12a9b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNNs on All Magnifications\n",
    "cnn_results = {}\n",
    "\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    train_loader = datasets_all_mags[mag]['train_loader']\n",
    "    val_loader = datasets_all_mags[mag]['val_loader']\n",
    "    test_loader = datasets_all_mags[mag]['test_loader']\n",
    "\n",
    "    model = BreastCancerCNN().to(device)\n",
    "\n",
    "    dataset = datasets_all_mags[mag]['dataset']\n",
    "    benign_count = dataset.labels.count(0)\n",
    "    malignant_count = dataset.labels.count(1)\n",
    "    class_weights = torch.tensor([1.0/benign_count, 1.0/malignant_count]).to(device)\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(20):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels_list.extend(labels.numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_labels_list, val_preds)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_preds.extend(predicted.cpu().numpy())\n",
    "            test_labels_list.extend(labels.numpy())\n",
    "\n",
    "    cnn_results[mag] = {\n",
    "        'accuracy': accuracy_score(test_labels_list, test_preds),\n",
    "        'precision': precision_score(test_labels_list, test_preds),\n",
    "        'recall': recall_score(test_labels_list, test_preds),\n",
    "        'f1': f1_score(test_labels_list, test_preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745570e-c207-444c-81a0-04a19793d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Results\n",
    "all_data = {}\n",
    "\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    all_data[mag] = {}\n",
    "    for model_name in ['Logistic Regression', 'Linear SVM', 'RBF SVM', 'Random Forest']:\n",
    "        all_data[mag][model_name] = {\n",
    "            'F1': all_results[mag][model_name]['f1'],\n",
    "            'Accuracy': all_results[mag][model_name]['accuracy'],\n",
    "            'Recall': all_results[mag][model_name]['recall'],\n",
    "            'Precision': all_results[mag][model_name]['precision']\n",
    "        }\n",
    "    all_data[mag]['CNN'] = {\n",
    "        'F1': cnn_results[mag]['f1'],\n",
    "        'Accuracy': cnn_results[mag]['accuracy'],\n",
    "        'Recall': cnn_results[mag]['recall'],\n",
    "        'Precision': cnn_results[mag]['precision']\n",
    "    }\n",
    "\n",
    "results_data = []\n",
    "for mag in ['40X', '100X', '200X', '400X']:\n",
    "    for model in ['Logistic Regression', 'Linear SVM', 'RBF SVM', 'Random Forest', 'CNN']:\n",
    "        results_data.append({\n",
    "            'Magnification': mag,\n",
    "            'Model': model,\n",
    "            'F1': all_data[mag][model]['F1'],\n",
    "            'Accuracy': all_data[mag][model]['Accuracy'],\n",
    "            'Recall': all_data[mag][model]['Recall'],\n",
    "            'Precision': all_data[mag][model]['Precision']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b29541-7aed-4800-ae01-f297aa83b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sns.barplot(data=results_df, x='Magnification', y='F1', hue='Model', ax=axes[0,0], palette='Set2')\n",
    "axes[0,0].set_title('(a) F1 Score by Magnification', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylim(0.8, 1.0)\n",
    "axes[0,0].legend(fontsize=8, loc='lower right')\n",
    "\n",
    "sns.barplot(data=results_df, x='Magnification', y='Accuracy', hue='Model', ax=axes[0,1], palette='Set2')\n",
    "axes[0,1].set_title('(b) Accuracy by Magnification', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylim(0.8, 1.0)\n",
    "axes[0,1].legend(fontsize=8, loc='lower right')\n",
    "\n",
    "sns.barplot(data=results_df, x='Magnification', y='Recall', hue='Model', ax=axes[1,0], palette='Set2')\n",
    "axes[1,0].set_title('(c) Recall by Magnification', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylim(0.8, 1.0)\n",
    "axes[1,0].legend(fontsize=8, loc='lower right')\n",
    "\n",
    "best_per_mag = results_df.loc[results_df.groupby('Magnification')['F1'].idxmax()]\n",
    "axes[1,1].bar(best_per_mag['Magnification'], best_per_mag['F1'], \n",
    "              color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'], edgecolor='black', linewidth=1.5)\n",
    "axes[1,1].set_title('(d) Best Model per Magnification', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylim(0.8, 1.0)\n",
    "for i, (mag, f1, model) in enumerate(zip(best_per_mag['Magnification'], best_per_mag['F1'], best_per_mag['Model'])):\n",
    "    axes[1,1].text(i, f1 + 0.005, f'{f1:.4f}\\n{model}', ha='center', fontsize=7, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/figure2_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = np.array([[82, 9], [0, 209]])\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Malignant'],\n",
    "            yticklabels=['Benign', 'Malignant'],\n",
    "            annot_kws={'size': 14, 'weight': 'bold'},\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix: 100X RBF SVM', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.text(2.6, 0.5, 'Accuracy: 97.14%\\nPrecision: 96.05%\\nRecall: 100.00%\\nF1: 97.98%', \n",
    "         fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/figure3_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4e405-2303-49d6-a8d6-f0f7216ea9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: During initial run, 200X data was accidentally identical to 400X due to \n",
    "# a data loading error. The issue has been fixed in Cells 6-12 above, but to avoid\n",
    "# re-running the entire notebook (which takes 2+ hours), I reran everything for 200x, which is the code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2889d5c-a6bb-4f48-910c-6d893dc1902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "base_path = '/content/drive/MyDrive/archive (2)/BreaKHis_v1/BreaKHis_v1/histology_slides/breast'\n",
    "\n",
    "class BreastCancerDataset(Dataset):\n",
    "    def __init__(self, base_path, magnification='400X', transform=None):\n",
    "        self.base_path = base_path\n",
    "        self.magnification = magnification\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, label in [('benign', 0), ('malignant', 1)]:\n",
    "            class_path = os.path.join(base_path, class_name)\n",
    "            \n",
    "            for root, dirs, files in os.walk(class_path):\n",
    "                if magnification in root:\n",
    "                    for file in files:\n",
    "                        if file.endswith('.png'):\n",
    "                            img_path = os.path.join(root, file)\n",
    "                            self.images.append(img_path)\n",
    "                            self.labels.append(label)\n",
    "        \n",
    "        print(f\"{magnification}: {len(self.images)} images ({self.labels.count(0)} benign, {self.labels.count(1)} malignant)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_200X = BreastCancerDataset(base_path, magnification='200X', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bb648-51d9-4e22-bcc0-bea676df47ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset_200X)))\n",
    "labels = dataset_200X.labels\n",
    "\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.30, random_state=42, stratify=labels)\n",
    "temp_labels = [labels[i] for i in temp_idx]\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=42, stratify=temp_labels)\n",
    "\n",
    "train_dataset = Subset(dataset_200X, train_idx)\n",
    "val_dataset = Subset(dataset_200X, val_idx)\n",
    "test_dataset = Subset(dataset_200X, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca51616-7a77-4c5d-a728-003c187a9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(weights='IMAGENET1K_V1')\n",
    "feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "feature_extractor = feature_extractor.to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "def extract_features(dataloader, extractor):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            feats = extractor(images)\n",
    "            feats = feats.view(feats.size(0), -1)\n",
    "            features.append(feats.cpu().numpy())\n",
    "            labels.append(lbls.numpy())\n",
    "    \n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels\n",
    "\n",
    "X_train_200X, y_train_200X = extract_features(train_loader, feature_extractor)\n",
    "X_val_200X, y_val_200X = extract_features(val_loader, feature_extractor)\n",
    "X_test_200X, y_test_200X = extract_features(test_loader, feature_extractor)\n",
    "\n",
    "print(f\"Train: {X_train_200X.shape}, Val: {X_val_200X.shape}, Test: {X_test_200X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a7eb3-e26a-4419-8b4c-a4a521b214e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ALL 200X MODELS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import time\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "models_params = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l2']\n",
    "        }\n",
    "    },\n",
    "    'Linear SVM': {\n",
    "        'model': SVC(kernel='linear', class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "    },\n",
    "    'RBF SVM': {\n",
    "        'model': SVC(kernel='rbf', class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'gamma': ['scale', 0.001, 0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mag = '200X'\n",
    "mag_results_200X = {}\n",
    "\n",
    "for name, config in models_params.items():\n",
    "    print(f\"\\n{name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        config['model'], \n",
    "        config['params'], \n",
    "        cv=5, \n",
    "        scoring=f1_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_200X, y_train_200X)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_200X)\n",
    "    \n",
    "    mag_results_200X[name] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy_score(y_test_200X, y_pred),\n",
    "        'precision': precision_score(y_test_200X, y_pred),\n",
    "        'recall': recall_score(y_test_200X, y_pred),\n",
    "        'f1': f1_score(y_test_200X, y_pred),\n",
    "        'train_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  F1: {mag_results_200X[name]['f1']:.4f}\")\n",
    "    print(f\"  Accuracy: {mag_results_200X[name]['accuracy']:.4f}\")\n",
    "    print(f\"  Recall: {mag_results_200X[name]['recall']:.4f}\")\n",
    "    print(f\"  Time: {mag_results_200X[name]['train_time']:.1f}s\")\n",
    "\n",
    "for model_name in ['Logistic Regression', 'Linear SVM', 'RBF SVM', 'Random Forest']:\n",
    "    res = mag_results_200X[model_name]\n",
    "    print(f\"{model_name:20s}: F1={res['f1']:.4f}, Acc={res['accuracy']:.4f}, Recall={res['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ecffa7-1da7-4277-b7b5-f6a5357e967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN 200X CNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class BreastCancerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BreastCancerCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = BreastCancerCNN().to(device)\n",
    "\n",
    "benign_count = 623\n",
    "malignant_count = 1390\n",
    "class_weights = torch.tensor([1.0/benign_count, 1.0/malignant_count]).to(device)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_preds.extend(predicted.cpu().numpy())\n",
    "            val_labels_list.extend(labels.numpy())\n",
    "    \n",
    "    val_f1 = f1_score(val_labels_list, val_preds)\n",
    "    val_acc = accuracy_score(val_labels_list, val_preds)\n",
    "    \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss/len(train_loader):.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_preds.extend(predicted.cpu().numpy())\n",
    "        test_labels_list.extend(labels.numpy())\n",
    "\n",
    "cnn_acc = accuracy_score(test_labels_list, test_preds)\n",
    "cnn_prec = precision_score(test_labels_list, test_preds)\n",
    "cnn_rec = recall_score(test_labels_list, test_preds)\n",
    "cnn_f1 = f1_score(test_labels_list, test_preds)\n",
    "\n",
    "print(f\"\\n200X CNN Test Results:\")\n",
    "print(f\"  Accuracy:  {cnn_acc:.4f}\")\n",
    "print(f\"  Precision: {cnn_prec:.4f}\")\n",
    "print(f\"  Recall:    {cnn_rec:.4f}\")\n",
    "print(f\"  F1 Score:  {cnn_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ML)",
   "language": "python",
   "name": "ml310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
